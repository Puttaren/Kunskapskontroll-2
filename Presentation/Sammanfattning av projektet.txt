Kunskapskontroll 2: MNIST-modellering med tillhörande Streamlit-app
Michael Broström

Det här har varit ett mycket lärorikt och intressant projekt. Det har tagit många och långa timmar, 
men har hela tiden varit väldigt roligt. Utom när man suttit och väntat på modellens träning. 


Struktur
Hela projektet är nu uppbyggt som en komplett pipeline för maskininlärning, från inläsning av rådata till logisk verifiering och en användardriven förbättring som kan leva vidare och bidra till en bättre och bättre modell.

Modellering
Efter att ha funderat lite på hur jag skulle komma igång så tänkte jag att det bästa måste vara att bygga en stabil bas genom att fokusera på grundflödet i maskininlärning. 

Jag tittade igenom föreläsningarna igen och lät bokens arbetsgång vara stommen för min egen hantering i början. Liksom i boken blev valet en standard-SVC-modell (Support Vector Classifier) och eftersom jag hade "tjuvstartat" så blev den utan någon dimensionsreducering (som vi då inte hade gått igenom än). 

För bästa möjliga datahantering normaliserade jag alla pixlar (784 features) genom att helt sonika dela dem med 255.0, vilket undanröjde behovet av StandardScaler.

Efter en hel del experimenterande och "skruvande på alla rattar" blev resultatet en färdig produktionsmodell med en accuracy på 0.9834. Det fick räknas som godkänt och jag gick därför vidare till preprocessing och prediktion. 

Fortsatt modellering
När appen var klar gick jag dock tillbaka för att jaga bättre siffror. En Accuracy på 99% borde inte vara omöjlig även utan neuralt nätverk. 

Från den stabila baslinje jag hade i min första "färdiga" modellering, en SVC med en accuracy på 0.9834 fick jag förnyad motivation när mina kamrater fick värden upppåt 0.985 och högre. Jag satte därför på mig tänkarhatten och körde vidare. 

1. Jag hade inte tidigare använt PCA så det var första steget. Jag experimenterade också med standardscaler, men återgick ganska snart till mitt "trick" att normalisera data genom att dela dem med 255.0 - det bevarar "tystnaden" i bakgrunden och var gynnsamt för dimensionsreduceringen. 

2. De 70 000 MNIST-bilderna delades 80/20 med stratifiering för att bibehålla samma klassfördelning i alla steg.

3. Jag experimenterade mycket med olika modeller, med voting och allsköns alternativ, men det visade sig hela tiden att SVC med rbf-kärna var den starkaste modellen. Med hjälp av dimensionsreducering (PCA) hanterade jag det som i boken och på en föreläsning kallades "the curse of dimensionality". Diverse analyser pekade på en kraftig reducering av dimensioner och efter diverse bildbehandling och handpåläggning visade det sig att 112 komponenter var "sweetspot". 

4. Därefter skapades en pipeline-arkitektur som mall, eller "single source of truth", för att säkerställa att testdata skulle transformeras exakt likadant som träningsdata.

5. SVC med PCA var bra och gav fina siffror, men det som verkligen lyfte modelleringen var en avancerad bildbearbetning av underlaget. 
   - Deskewing (Upprätning): Avancerad matematik användes för at beräkna bildens "moment" och räta upp siffror som är skrivna för snett. Detta eliminerar lutning som felkälla och gör att modellen kan fokusera på siffrans form.
   - Ultra-augmentation: Här var en riktig nyckel till bättre accuracy. Genom att använda skiftningar (1 pixel åt fyra håll), olika rotationer och  grader samt zoom (0,9 och 1,1) utökades de befintliga träningsdata från 56 000 till 616 000 rader!
   - Feature Extraction (HOG): Det här var ett sidospår som jag experimenterade med, men det visade sig att obearbetade pixlar med PCA presterade bättre på min nu mycket större datamängd.

6. Vad blev då resultatet? Min accuracy gick via mina olika experiment sakta uppåt och till slut landade det på 0.9928 - det innebär att bland de 14 000 testbilderna gissade modellen fel endast 101 gånger! 

Jakten på de sista tusendelarna bedrevs bl.a. via en TTA-hantering (Test-Time Augmentation). När det till slut var klart och min dator nästan tröttnat på alla beräkningar kom jag fram till high score: En accuracy på hela 0.9934 - bara 92 bilder i hela testsetet identifieras fel och det är för att de är riktigt dåliga (kolla själv, exempel finns här i mappstrukturen). 




### `preprocess.py` – Bildbehandlingsmotorn

Detta är projektets tekniska kärna som översätter verkliga bilder till MNIST-standard genom flera steg:

* **Smart bakgrundsanalys:** Analyserar bildens kanter för att avgöra om det finns skuggor eller brus, och justerar ljusstyrka/kontrast därefter.
* **Isolering & Topologi:** Använder `ndimage.label` för att räkna sammanhängande pixlar ("blobs"), vilket hjälper till att upptäcka om användaren ritar flera figurer.
* **Geometrisk normalisering:** Hittar siffrans exakta ytterkanter, skalar den till  pixlar och placerar den i en -ram.
* **Centrering (Center of Mass):** Finjusterar siffrans position baserat på dess tyngdpunkt för att efterlikna MNIST-datasetets struktur.
* **"MNIST-ifiering":** Applicerar Gaussisk oskärpa och autokontrast för att ge råbilden samma karaktär som träningsdatan.

### `predict.py` – Gränssnitt & Logik

Scriptet fungerar som projektets ansikte utåt och hanterar interaktionen via Streamlit:

* **Inmatningsmetoder:** Erbjuder både digital rityta (`st_canvas`) och filuppladdare.
* **TTA – Test Time Augmentation (Juryn):** För varje prediktion skapas 20 slumpmässiga varianter (rotation/skiftning) av bilden. Modellen röstar på samtliga, och resultatet presenteras som en "jury-enighet" i procent.
* **Session Management:** Använder `st.session_state` för att behålla resultat även vid rensning av ritytan.
* **Human-in-the-loop:** Innehåller en feedback-sektion där användaren kan korrigera modellen och spara problembilder för framtida träning.

---

## 2. Modellering & Optimering

### Val av Algoritm

Valet föll på en **Support Vector Classifier (SVC)** med RBF-kernel.

* **Datavolym:** Modellen är tränad på ca 735 000 rader, bestående av original-MNIST kombinerat med omfattande augmentering.
* **Träningstid:** Den slutgiltiga träningen tog 151 minuter, en medveten investering för att uppnå en extremt robust beslutsgräns i ett högdimensionellt rum.

### Heuristik & Logisk Verifiering

För att hantera gränsfall där den rent matematiska modellen tvekar, har en logisk kontrollinstans implementerats i `predict.py`:

* **Hål-detektering:** Om modellen gissar på en 5:a men topologin visar ett slutet hål, korrigeras resultatet till en 6:a.
* **Geometriska varningar:** Systemet varnar om en figur är misstänkt smal eller om flera oberoende figurer detekteras.

---

## 3. Active Learning: "The 6-Problem"

En central del av arbetet har varit att identifiera och åtgärda specifika svagheter i modellen:

* **Felanalys:** Genom en Confusion Matrix identifierades att handskrivna 6:or (utan tydligt "skaft") ofta misstogs för 5:or.
* **Data-injektion:** 67 unika problembilder (egna ritningar och MNIST-fel) samlades in och blåstes upp till 6 700 nya träningsexempel genom super-augmentering.
* **Resultat:** Efter omträning minskade antalet "6 som 5"-fel i testsetet från 2 till 1, och modellens säkerhet på användarens specifika handstil ökade till 100 % jury-enighet.

---

## 4. Framtida Datapersistens

För att projektet ska kunna skala online har ett vägval gjorts gällande lagring:

* **Problem:** Lokala filer raderas vid omstart av Streamlit Cloud-servrar.
* **Lösning:** Implementering av koppling till **Google Sheets** för att spara insamlade bilder som Base64-strängar. Detta möjliggör permanent lagring av användardata för kontinuerlig förbättring av modellen utanför den temporära servermiljön.

---

**Är det någon specifik del du vill att jag fördjupar mig i, eller vill du att jag tar fram koden för Google Sheets-kopplingen så att vi kan bocka av punkt 4 i verkligheten?**